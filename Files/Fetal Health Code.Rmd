---
title: "Classification - HW4"
author: "Enes Ata Oru√ß"
date: "29.01.2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
  The aim of this homework is to compare the performance of penalized regression approaches, decision trees, support vector machines and tree-based ensembles.
We are asked to find 4 datasets for a classification task from different domains. This report is created for Fetal Health Dataset.

### Fetal Health Data: ### (https://www.kaggle.com/andrewmvd/fetal-health-classification)
  This dataset contains 2126 observations and 22 features extracted from Cardiotocogram exams, which were then classified by three expert obstetritians into 3 classes:

1 - Normal
2 - Suspect
3 - Pathological
  

```{r, message=FALSE, warning=FALSE}
library(caTools)
library(randomForest)
library(ROCR)
library(tidyverse)
library(data.table)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(mlbench)
library(MLmetrics)
library(dplyr)
library(caret)
library(readr)
```

## 1. Penalized Regression Approach: ##

```{r}
health_fet <- fread("fetal_health.csv")

library(glmnet)
health_fet$fetal_health <- as.numeric(health_fet$fetal_health)
smp_size_pra <- floor((2/3) * nrow(health_fet))

# set the seed to make your partition reproducible
set.seed(1234)
train_ind_pra <- sample(seq_len(nrow(health_fet)), size = smp_size_pra)

train_pra <- health_fet[train_ind_pra, ]
test_pra <- health_fet[-train_ind_pra, ]


health_mat_2 <- data.matrix(train_pra)
cvfit=cv.glmnet(health_mat_2, train_pra$fetal_health,family='gaussian',nfolds=10)
cvfit
lambda_min <- cvfit$lambda.min
lambda_min
# Lasso with Train Set
lassofit <- glmnet(x=health_mat_2, y = train_pra$fetal_health, alpha = 1,lambda = lambda_min)
lasso_train_predicted <-as.numeric(predict(lassofit, newx=data.matrix(train_pra)))

# RMSE for Train Lasso
caret::RMSE(lasso_train_predicted, train_pra$fetal_health)

# Lasso with Test Set
lasso_test_predicted <-as.numeric(predict(lassofit, newx=data.matrix(test_pra)))
# RMSE for Test Lasso
caret::RMSE(lasso_test_predicted, test_pra$fetal_health)

```

## 2. Decision Tree Approach: ##

```{r health_fet, message=FALSE, warning=FALSE}
# 75% of the sample size health_fet
health_fet$fetal_health <- as.numeric(health_fet$fetal_health)
smp_size_dt <- floor((2/3) * nrow(health_fet))

# set the seed to make your partition reproducible
set.seed(123)
train_ind_dt <- sample(seq_len(nrow(health_fet)), size = smp_size_dt)

train_dt_health <- health_fet[train_ind_dt, ]
test_dt_health <- health_fet[-train_ind_dt, ]

#method is "class" because I want to perform classification
mytree_health <- rpart(
  fetal_health ~ ., 
  data = train_dt_health, 
  method = "class"
)

# plot mytree
fancyRpartPlot(mytree_health, caption = NULL)

printcp(mytree_health) # display the results 
plotcp(mytree_health) # visualize cross-validation results 
summary(mytree_health) # detailed summary of splits

#prediction
predictions <- predict(mytree_health, test_dt_health, type = "class")
predictions = as.numeric(predictions)
# Prediction measures
table(test_dt_health$fetal_health, predictions)
accuracy=(530+57+58)/nrow(test_dt_health)
accuracy
#RMSE Value for performance test
RMSE(test_dt_health$fetal_health,predictions)
```

## 3. Random Forest Approach on Fetal Health Dataset: ##

```{r}
#Random Forest Approach
health_fet$fetal_health = as.factor(health_fet$fetal_health)
#Test/train split
smp_size <- floor((2/3) * nrow(health_fet))

# set the seed to make your partition reproducible
set.seed(1234)
train_ind <- sample(seq_len(nrow(health_fet)), size = smp_size)
train_rf <- health_fet[train_ind, ]
test_rf <- health_fet[-train_ind, ]
x <- train_rf[,1:21]
y <- train_rf[,22] 
#RF Implement
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 5
metric <- "Accuracy"
set.seed(1234)
mtry <- sqrt(ncol(x))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(fetal_health~., data=train_rf, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
# accuracy calculation of prediction
predictForest_2 = predict(rf_default, newdata = test_rf)
table(test_rf$fetal_health, predictForest_2)
accuracy=(542+74+48)/nrow(test_rf)
accuracy
test_rf$fetal_health<-as.numeric(test_rf$fetal_health)
#RMSE Value for performance test
predictForest_2 <- as.numeric(predictForest_2)
RMSE(test_rf$fetal_health,predictForest_2)
```

## 4. Stochastic Gradient Approach: ##

```{r sga, message=FALSE, warning=FALSE}
library(gbm)
health_fet$fetal_health = as.numeric(health_fet$fetal_health)
#Test/train split
smp_size <- floor((2/3) * nrow(health_fet))

# set the seed to make your partition reproducible
set.seed(1234)

train_ind <- sample(seq_len(nrow(health_fet)), size = smp_size)
train_sgb <- health_fet[train_ind, ]
test_sgb <- health_fet[-train_ind, ]
# randomize data
random_index <- sample(1:nrow(train_sgb), nrow(train_sgb))
random_sgb_train <- train_sgb[random_index, ]

# modify hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05, .1),
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 7, 10),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)
## [1] 81

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(formula = fetal_health ~ .,
    distribution = "gaussian",
    data = random_sgb_train,
    n.trees = 600,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)

# for reproducibility
set.seed(123)

# train GBM model
gbm.fit.final <- gbm(formula = fetal_health ~ .,
  distribution = "gaussian",
  data = train_sgb,
  n.trees = 483,
  interaction.depth = 5,
  shrinkage = 0.1,
  n.minobsinnode = 5,
  bag.fraction = .65, 
  train.fraction = 1,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
# predict values for test data
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, test_sgb)

# results
caret::RMSE(pred, test_sgb$fetal_health)
```

## Compare Results ##
Penalized Regression Approach
```{r compare pra, message=FALSE, warning=FALSE}
# RMSE for Test Lasso
caret::RMSE(lasso_test_predicted, test_pra$fetal_health)
```
Decision Tree Approach
```{r compare DT, message=FALSE, warning=FALSE}
# Prediction measures DT
table(test_dt_health$fetal_health, predictions)
accuracy=(530+57+58)/nrow(test_dt_health)
accuracy
#RMSE Value for performance test DT
RMSE(test_dt_health$fetal_health,predictions)
```
Random Forest
```{r compare rf, message=FALSE, warning=FALSE}
#RMSE Value for performance test
predictForest_2 <- as.numeric(predictForest_2)
RMSE(test_rf$fetal_health,predictForest_2)
```
Stochastic Gradient Boosting
```{r compare sgb, message=FALSE, warning=FALSE}
# results
caret::RMSE(pred, test_sgb$fetal_health)
```


