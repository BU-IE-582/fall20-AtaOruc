---
title: "**Homework 4 Classification**"
author: "Enes Ata Oru√ß"
date: "29.01.2021"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
  The aim of this homework is to compare the performance of penalized regression approaches, decision trees, support vector machines and tree-based ensembles.
We are asked to find 4 datasets for a classification task from different domains. This report is created for Absenteeism Dataset.

### Absenteeism Data:  (https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work)
  This dataset contains 740 observations and 21 features extracted from Cardiotocogram exams. Target column determined by the owners of the data. The target column is absent_hrs. This represents Absenteeism time in hours.
  

```{r, message=FALSE, warning=FALSE}
library(caTools)
library(randomForest)
library(ROCR)
library(tidyverse)
library(data.table)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(mlbench)
library(MLmetrics)
library(dplyr)
library(caret)
library(readr)
```

## 1. Penalized Regression Approach: ##

```{r pra}
absent <- fread("Absenteeism.csv")

library(glmnet)
absent$absent_hrs <- as.numeric(absent$absent_hrs)
smp_size_pra <- floor((2/3) * nrow(absent))

# set the seed to make your partition reproducible
set.seed(1234)
train_ind_pra <- sample(seq_len(nrow(absent)), size = smp_size_pra)

train_pra <- absent[train_ind_pra, ]
test_pra <- absent[-train_ind_pra, ]


absent_mat_2 <- data.matrix(train_pra)
cvfit=cv.glmnet(absent_mat_2, train_pra$absent_hrs,family='gaussian',nfolds=10)
cvfit
lambda_min <- cvfit$lambda.min
lambda_min
# Lasso with Train Set
lassofit <- glmnet(x=absent_mat_2, y = train_pra$absent_hrs, alpha = 1,lambda = lambda_min)
lasso_train_predicted <-as.numeric(predict(lassofit, newx=data.matrix(train_pra)))

# RMSE for Train Lasso
caret::RMSE(lasso_train_predicted, train_pra$absent_hrs)

# Lasso with Test Set
lasso_test_predicted <-as.numeric(predict(lassofit, newx=data.matrix(test_pra)))
# RMSE for Test Lasso
caret::RMSE(lasso_test_predicted, test_pra$absent_hrs)

```

## 2. Decision Tree Approach: ##

```{r absent, message=FALSE, warning=FALSE}
# 75% of the sample size absent
absent$absent_hrs <- as.numeric(absent$absent_hrs)
smp_size_dt <- floor((2/3) * nrow(absent))

# set the seed to make your partition reproducible
set.seed(123)
train_ind_dt <- sample(seq_len(nrow(absent)), size = smp_size_dt)

train_dt_absent <- absent[train_ind_dt, ]
test_dt_absent <- absent[-train_ind_dt, ]

#method is "class" because I want to perform classification
mytree_absent <- rpart(
  absent_hrs ~ ., 
  data = train_dt_absent, 
  method = "class"
)

# plot mytree
fancyRpartPlot(mytree_absent, caption = NULL)

printcp(mytree_absent) # display the results 
plotcp(mytree_absent) # visualize cross-validation results 
summary(mytree_absent) # detailed summary of splits

#prediction
predictions <- predict(mytree_absent, test_dt_absent, type = "class")
predictions = as.numeric(predictions)
# Prediction measures
table(test_dt_absent$absent_hrs, predictions)
#RMSE Value for performance test
RMSE(test_dt_absent$absent_hrs,predictions)
```

## 3. Random Forest Approach on Absenteeism Dataset: ##

```{r}
#Random Forest Approach
absent$absent_hrs = as.numeric(absent$absent_hrs)
#Test/train split
smp_size <- floor((2/3) * nrow(absent))

# set the seed to make your partition reproducible
set.seed(1234)

train_ind <- sample(seq_len(nrow(absent)), size = smp_size)
train_rf <- absent[train_ind, ]
test_rf <- absent[-train_ind, ]
# randomize data
random_index <- sample(1:nrow(train_rf), nrow(train_rf))
random_rf_train <- train_rf[random_index, ]

# Fit random forest: model
# Set seed
set.seed(33)
# Fit a model
model <- train(absent_hrs~.,
               data = random_rf_train,
               method = "ranger",
               trControl = trainControl(method = "cv", number = 5)
                             )
# Let's check the model
model
fit <- predict(model, newdata = test_rf)
fit_model <- train(absent_hrs~.,
               data = test_rf,
               method = "ranger",
               trControl = trainControl(method = "cv", number = 5)
                            )
fit_model
test_rf$absent_hrs <- as.numeric(test_rf$absent_hrs)
# RMSE for measure error
RMSE(test_rf$absent_hrs,fit)
```

## 4. Stochastic Gradient Approach: ##

```{r sga, message=FALSE, warning=FALSE}
library(gbm)
absent$absent_hrs = as.numeric(absent$absent_hrs)
#Test/train split
smp_size <- floor((2/3) * nrow(absent))

# set the seed to make your partition reproducible
set.seed(1234)

train_ind <- sample(seq_len(nrow(absent)), size = smp_size)
train_sgb <- absent[train_ind, ]
test_sgb <- absent[-train_ind, ]
# randomize data
random_index <- sample(1:nrow(train_sgb), nrow(train_sgb))
random_sgb_train <- train_sgb[random_index, ]

# modify hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05, .1),
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 7, 10),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)
## [1] 81

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(formula = absent_hrs ~ .,
    distribution = "gaussian",
    data = random_sgb_train,
    n.trees = 600,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)

# for reproducibility
set.seed(123)

# train GBM model
gbm.fit.final <- gbm(formula = absent_hrs ~ .,
  distribution = "gaussian",
  data = train_sgb,
  n.trees = 483,
  interaction.depth = 5,
  shrinkage = 0.1,
  n.minobsinnode = 5,
  bag.fraction = .65, 
  train.fraction = 1,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
# predict values for test data
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, test_sgb)

# results
caret::RMSE(pred, test_sgb$absent_hrs)
```

## Compare Results ##
Penalized Regression Approach
```{r compare pra, message=FALSE, warning=FALSE}
# RMSE for Test Lasso
caret::RMSE(lasso_test_predicted, test_pra$absent_hrs)
```
Decision Tree Approach
```{r compare DT, message=FALSE, warning=FALSE}
#RMSE Value for performance test DT
RMSE(test_dt_absent$absent_hrs,predictions)
```
Random Forest
```{r compare rf, message=FALSE, warning=FALSE}
# RMSE Value for performance test
RMSE(test_rf$absent_hrs,fit)
```
Stochastic Gradient Boosting
```{r compare sgb, message=FALSE, warning=FALSE}
# results
caret::RMSE(pred, test_sgb$absent_hrs)
```

  When we looked at the RMSE values I believe there is some miscalculation happened in lasso approach because its value doesn't seem realistic. This lasso situation also happened on different datasets. But I tuned the other 3 approaches. Because of this meaningful assumptions, I believe they gave more reliable results. 

  So, when we consider the other approaches Random Forest gave the best classification result on Absenteeism data. There was a lot of different classes in Target colum (absent_hrs). This target column determined by the researches that provided this dataset. 










